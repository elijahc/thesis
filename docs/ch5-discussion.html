<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>CH 6 Single Units to Brain-wide States | Computational Models of Neural Encoding in Vision and Neurostimulation</title>
  <meta name="description" content="CH 6 Single Units to Brain-wide States | Computational Models of Neural Encoding in Vision and Neurostimulation" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="CH 6 Single Units to Brain-wide States | Computational Models of Neural Encoding in Vision and Neurostimulation" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="CH 6 Single Units to Brain-wide States | Computational Models of Neural Encoding in Vision and Neurostimulation" />
  
  
  

<meta name="author" content="Elijah Christensen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-jov.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="templates/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">elijahc / thesis</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="list-of-abbreviations.html"><a href="list-of-abbreviations.html"><i class="fa fa-check"></i>List of Abbreviations</a></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#sec:dbs"><i class="fa fa-check"></i><b>1.1</b> Deep Brain Stimulation</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#sec:neuralinterfaces"><i class="fa fa-check"></i><b>1.2</b> Neural Interfaces</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#sec:ch1summary"><i class="fa fa-check"></i><b>1.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html"><i class="fa fa-check"></i><b>2</b> Neuroscience and ML</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:artificialunits"><i class="fa fa-check"></i><b>2.1</b> Artificial Units</a></li>
<li class="chapter" data-level="2.2" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:layers"><i class="fa fa-check"></i><b>2.2</b> Layers</a></li>
<li class="chapter" data-level="2.3" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:modarchetypes"><i class="fa fa-check"></i><b>2.3</b> Model Archetypes</a></li>
<li class="chapter" data-level="2.4" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:architectures"><i class="fa fa-check"></i><b>2.4</b> Layer Architectures</a></li>
<li class="chapter" data-level="2.5" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:lossfunc"><i class="fa fa-check"></i><b>2.5</b> Loss Function</a></li>
<li class="chapter" data-level="2.6" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:learningrules"><i class="fa fa-check"></i><b>2.6</b> Learning Rules</a></li>
<li class="chapter" data-level="2.7" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:optimizers"><i class="fa fa-check"></i><b>2.7</b> Optimizers</a></li>
<li class="chapter" data-level="2.8" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:ch2summary"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-jsr.html"><a href="ch-jsr.html"><i class="fa fa-check"></i><b>3</b> Predicting sleep state in human PD patients</a>
<ul>
<li class="chapter" data-level="" data-path="ch-jsr.html"><a href="ch-jsr.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="3.1" data-path="ch-jsr.html"><a href="ch-jsr.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ch-jsr.html"><a href="ch-jsr.html#methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a></li>
<li class="chapter" data-level="3.3" data-path="ch-jsr.html"><a href="ch-jsr.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-vae.html"><a href="ch-vae.html"><i class="fa fa-check"></i><b>4</b> Models of the ventral stream that visualize and categorize images</a>
<ul>
<li class="chapter" data-level="" data-path="ch-vae.html"><a href="ch-vae.html#abstract-1"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="4.1" data-path="ch-vae.html"><a href="ch-vae.html#materials-and-methods"><i class="fa fa-check"></i><b>4.1</b> Materials and Methods</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-jov.html"><a href="ch-jov.html"><i class="fa fa-check"></i><b>5</b> Predicting single unit responses in macaque V1</a>
<ul>
<li class="chapter" data-level="" data-path="ch-jov.html"><a href="ch-jov.html#abstract-2"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="5.1" data-path="ch-jov.html"><a href="ch-jov.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="ch-jov.html"><a href="ch-jov.html#methods-1"><i class="fa fa-check"></i><b>5.2</b> Methods</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch5-discussion.html"><a href="ch5-discussion.html"><i class="fa fa-check"></i><b>6</b> Single Units to Brain-wide States</a>
<ul>
<li class="chapter" data-level="" data-path="ch5-discussion.html"><a href="ch5-discussion.html#abstract-3"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="6.1" data-path="ch5-discussion.html"><a href="ch5-discussion.html#modeling-neural-encoding-with-anns"><i class="fa fa-check"></i><b>6.1</b> Modeling Neural Encoding with ANN’s</a></li>
<li class="chapter" data-level="6.2" data-path="ch5-discussion.html"><a href="ch5-discussion.html#deep-brain-stimulation"><i class="fa fa-check"></i><b>6.2</b> Deep Brain Stimulation</a></li>
<li class="chapter" data-level="6.3" data-path="ch5-discussion.html"><a href="ch5-discussion.html#ann-models-are-useful-across-a-wide-range-of-physiologic-scales"><i class="fa fa-check"></i><b>6.3</b> ANN models are useful across a wide range of physiologic scales</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>REFERENCES</a></li>
<li class="divider"></li>
<li><strong><a href="https://github.com/rstudio/bookdown" target="blank">Proudly published with bookdown</a></strong></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Models of Neural Encoding in Vision and Neurostimulation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch5:discussion" class="section level1" number="6">
<h1><span class="header-section-number">CH 6</span> Single Units to Brain-wide States</h1>
<div id="abstract-3" class="section level2 unnumbered" number="">
<h2>Abstract</h2>
<p>A persisting challenge in neuroscience is to bridge the gap between the complex tasks we know brains can perform and the physical components (neurons) that enable them. In vision, this divide is particularly wide, and much effort has been devoted to understanding how our brain processes visual information. For instance, we know the visual cortex receives complex spatiotemporal patterns of light relayed by the retina and reformats these patterns to infer what caused them (i.e. the identity of the object present) <span class="citation">(DiCarlo, Zoccolan, and Rust <a href="references.html#ref-DiCarlo:2012em" role="doc-biblioref">2012</a>)</span>. Answering this question requires first understanding how visual information is encoded at each sequential stage of processing along brain areas in visual cortex. Computational modeling has much to offer neuroscience in addressing this knowledge gap. One approach is to build computational models to replicate the neural encoding which takes place when the brain receives sensory stimuli.</p>
</div>
<div id="modeling-neural-encoding-with-anns" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Modeling Neural Encoding with ANN’s</h2>
<div id="predicting-single-units" class="section level3" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Predicting Single Units</h3>
<p>At the most granular level, information is encoded in the spiking activities of individual neurons. Traditional systems neuroscience approaches have advanced our understanding neural encoding by providing explanations for what individual neurons compute. In visual systems neuroscience, Hubel and Wiesel performed the seminal work in the field showing that individual simple and complex cells in primary visual cortex (V1) “tuned” to respond to oriented edges. This approach has worked well for cells that respond well to simple stimuli, but the encoding properties of many other cells in V1 are still unexplained <span class="citation">(Olshausen and Field <a href="references.html#ref-Olshausen:2006fd" role="doc-biblioref">2006</a>)</span>.
We demonstrate ANN’s trained with machine learning techniques are a robust model of neural encoding in V1 capable of accurately predicting single neuron responses to natural stimuli. Importantly, this model achieves equally good predictability for both orientation selective and non-orientation selective cells for natural image stimuli. We show this approach is a useful tool for studying responses properties of previously difficult to study cells in V1.</p>
</div>
<div id="objectively-useful" class="section level3" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Objectively Useful</h3>
<p>David Marr proposed the idea that understanding the computational goals of visual processing is equally important and complementary to an understanding of the parts (e.g. individual neurons) that physically implement it. For early visual areas, efficient coding as a computational goal has been successful at explaining response properties of cells in primary visual cortex (V1) but has not worked as well at explaining responses in higher visual areas. The prevailing view for higher visual areas like inferior temporal cortex (IT) is that object recognition best describes its objective but directly testing this hypothesis is difficult.</p>
<p>Questions of this nature are fundamentally challenging to test, especially when limited to only analyzing responses for a handful of neurons. Recent results have demonstrated ANN’s may be better suited for evaluating higher visual area objectives. Deep convolutional neural networks (DCNN’s) trained to categorize objects in images also develop internal representations which also match IT responses to natural images. It has been posited that matching representations could only arise if both the model and ventral stream are optimized for the same objective. We tested this explanation by optimizing models for both image categorization and a composite categorize and reconstruct objective. We find models which optimize the composite objective have representations which IT better than categorize alone. This is surprising, if strictly object recognition best describes the objective of visual processing in the ventral stream, optimizing an alternate objective should develop more poorly matching representations. However, this finding may help reconcile two observations at odds with the strictly object recognition hypothesis. First, it’s been shown that visual processing areas show matching activation patterns in response to both viewing a scene and mentally visualizing the same scene <span class="citation">(Stokes et al. <a href="references.html#ref-Stokes:2009kg" role="doc-biblioref">2009</a>, @OCraven:2000th, @Freud:2016dj, @Sereno:2011hq)</span>. Second, ventral stream areas explicitly retain information not useful for object recognition<span class="citation">(Hong et al. <a href="references.html#ref-Hong:2016cw" role="doc-biblioref">2016</a>)</span>.</p>
<p>Half of nonhuman primate neocortex is devoted to visual processing <span class="citation">(Felleman and Van Essen <a href="references.html#ref-Felleman:1991tk" role="doc-biblioref">1991</a>)</span>, underscoring both its complexity and evolutionary importance. Furthermore, the ‘No free lunch theorems’ demonstrate objective function choice is not arbitrary; no learning algorithm performs well on all tasks. These pressures dictate an alternate objective choice, should have a compelling advantage. Our work suggests that an advantage such as noise robustness might explain why the alternate categorize and reconstruct objective provides a better match to IT representations.</p>
</div>
</div>
<div id="deep-brain-stimulation" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Deep Brain Stimulation</h2>
<p>Current neurostimulators are non-adaptive, limited by lack of robust methods for reading out a patient’s brain state necessary for adaptive neurostimulation. This work has also shown the promise of ANN’s as useful tool decoding information from neural activity. In chapter 3 we described our efforts to build such a model capable of detecting sleep stage from local field potential (LFP) recordings taken from DBS electrodes implanted in the subthalamic nucleus (STN). In this work we trained an ANN classifier model to predict the sleep state of PD patients from spectral decomposition features in their local field potentials.</p>
</div>
<div id="ann-models-are-useful-across-a-wide-range-of-physiologic-scales" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> ANN models are useful across a wide range of physiologic scales</h2>
<p>In this work we leverage ANN models as a powerful tool to improve our understanding of neural encoding, predicting brain-wide states, replicating population response properties in IT, and predicting individual neuron responses to stimuli. We demonstrate the objective best describing higher visual areas may be more complex than solely object recognition; and show why this may be important for practical reasons. While these findings have extended our understanding of visual encoding, there is still important work to be done to make more accurate models of cortical visual processing. For instance, the brain’s visual processing system is optimized to operate on sequences of images (e.g. video) and not just a single snapshot. The model’s described in this work do not take advantage of extra information present in the time domain of video and future models will likely take advantage of this kind of information.</p>

<!-- # --- -->
<!-- # chapter: 7 -->
<!-- # knit: "bookdown::render_book" -->
<!-- # --- -->
</div>
</div>
<script type="application/json" class="js-hypothesis-config">

{"showHighlights": "whenSidebarOpen"}

</script>

<script src="https://hypothes.is/embed.js" async></script>
            </section>

          </div>
        </div>
      </div>
<a href="ch-jov.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/elijahc/thesis/edit/master/Rmd//06-chap6.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
