<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>CH 4 Models of the ventral stream that visualize and categorize images | Computational Models of Neural Encoding in Vision and Neurostimulation</title>
  <meta name="description" content="CH 4 Models of the ventral stream that visualize and categorize images | Computational Models of Neural Encoding in Vision and Neurostimulation" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="CH 4 Models of the ventral stream that visualize and categorize images | Computational Models of Neural Encoding in Vision and Neurostimulation" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="CH 4 Models of the ventral stream that visualize and categorize images | Computational Models of Neural Encoding in Vision and Neurostimulation" />
  
  
  

<meta name="author" content="Elijah Christensen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-jsr.html"/>
<link rel="next" href="ch-jov.html"/>
<script src="libs/header-attrs-2.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="templates/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">elijahc / thesis</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="list-of-abbreviations.html"><a href="list-of-abbreviations.html"><i class="fa fa-check"></i>List of Abbreviations</a></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#sec:dbs"><i class="fa fa-check"></i><b>1.1</b> Deep Brain Stimulation</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#sec:neuralinterfaces"><i class="fa fa-check"></i><b>1.2</b> Neural Interfaces</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#sec:ch1summary"><i class="fa fa-check"></i><b>1.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html"><i class="fa fa-check"></i><b>2</b> Neuroscience and ML</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:artificialunits"><i class="fa fa-check"></i><b>2.1</b> Artificial Units</a></li>
<li class="chapter" data-level="2.2" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:layers"><i class="fa fa-check"></i><b>2.2</b> Layers</a></li>
<li class="chapter" data-level="2.3" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:modarchetypes"><i class="fa fa-check"></i><b>2.3</b> Model Archetypes</a></li>
<li class="chapter" data-level="2.4" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:architectures"><i class="fa fa-check"></i><b>2.4</b> Layer Architectures</a></li>
<li class="chapter" data-level="2.5" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:lossfunc"><i class="fa fa-check"></i><b>2.5</b> Loss Function</a></li>
<li class="chapter" data-level="2.6" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:learningrules"><i class="fa fa-check"></i><b>2.6</b> Learning Rules</a></li>
<li class="chapter" data-level="2.7" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:optimizers"><i class="fa fa-check"></i><b>2.7</b> Optimizers</a></li>
<li class="chapter" data-level="2.8" data-path="ch-mlprimer.html"><a href="ch-mlprimer.html#sec:ch2summary"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-jsr.html"><a href="ch-jsr.html"><i class="fa fa-check"></i><b>3</b> Predicting sleep state in human PD patients</a>
<ul>
<li class="chapter" data-level="" data-path="ch-jsr.html"><a href="ch-jsr.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="3.1" data-path="ch-jsr.html"><a href="ch-jsr.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ch-jsr.html"><a href="ch-jsr.html#methods"><i class="fa fa-check"></i><b>3.2</b> Methods</a></li>
<li class="chapter" data-level="3.3" data-path="ch-jsr.html"><a href="ch-jsr.html#results"><i class="fa fa-check"></i><b>3.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-vae.html"><a href="ch-vae.html"><i class="fa fa-check"></i><b>4</b> Models of the ventral stream that visualize and categorize images</a>
<ul>
<li class="chapter" data-level="" data-path="ch-vae.html"><a href="ch-vae.html#abstract-1"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="4.1" data-path="ch-vae.html"><a href="ch-vae.html#materials-and-methods"><i class="fa fa-check"></i><b>4.1</b> Materials and Methods</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-jov.html"><a href="ch-jov.html"><i class="fa fa-check"></i><b>5</b> Predicting single unit responses in macaque V1</a>
<ul>
<li class="chapter" data-level="" data-path="ch-jov.html"><a href="ch-jov.html#abstract-2"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="5.1" data-path="ch-jov.html"><a href="ch-jov.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="ch-jov.html"><a href="ch-jov.html#methods-1"><i class="fa fa-check"></i><b>5.2</b> Methods</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch5-discussion.html"><a href="ch5-discussion.html"><i class="fa fa-check"></i><b>6</b> Single Units to Brain-wide States</a>
<ul>
<li class="chapter" data-level="" data-path="ch5-discussion.html"><a href="ch5-discussion.html#abstract-3"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="6.1" data-path="ch5-discussion.html"><a href="ch5-discussion.html#modeling-neural-encoding-with-anns"><i class="fa fa-check"></i><b>6.1</b> Modeling Neural Encoding with ANN’s</a></li>
<li class="chapter" data-level="6.2" data-path="ch5-discussion.html"><a href="ch5-discussion.html#deep-brain-stimulation"><i class="fa fa-check"></i><b>6.2</b> Deep Brain Stimulation</a></li>
<li class="chapter" data-level="6.3" data-path="ch5-discussion.html"><a href="ch5-discussion.html#ann-models-are-useful-across-a-wide-range-of-physiologic-scales"><i class="fa fa-check"></i><b>6.3</b> ANN models are useful across a wide range of physiologic scales</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>REFERENCES</a></li>
<li class="divider"></li>
<li><strong><a href="https://github.com/rstudio/bookdown" target="blank">Proudly published with bookdown</a></strong></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Models of Neural Encoding in Vision and Neurostimulation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch:vae" class="section level1" number="4">
<h1><span class="header-section-number">CH 4</span> Models of the ventral stream that visualize and categorize images</h1>
<blockquote>
<p>The work presented in this chapter is not yet published, but a <a href="https://www.biorxiv.org/content/10.1101/2020.02.21.958488v1">preprint</a> is available (see below)</p>
</blockquote>
<ul>
<li><a href="vae.pdf">pdf</a></li>
</ul>
<div id="abstract-1" class="section level2 unnumbered" number="">
<h2>Abstract</h2>
<p>The ventral stream (VS) of visual cortex begins in primary visual cortex (V1), ends in inferior temporal cortex (IT), and is essential for object recognition.
Accordingly, the long-standing belief in the field is that the ventral stream could be understood as mapping visual scenes onto neuronal firing patterns that represent object identity(Felleman and Van Essen, 1991).
Supporting that assertion, deep convolutional neural networks (DCNN’s) trained to categorize objects in natural images develop intermediate representations that resemble those in primate VS(Cadieu et al., 2014; Güçlü and van Gerven, 2015; Yamins et al., 2014; Yamins and DiCarlo, 2016).
However, several recent findings appear at odds with the object recognition hypothesis.
VS and other visual areas are also engaged during visualization of both prior experience and novel scenes (O’Craven and Kanwisher, 2006; Stokes et al., 2009), suggesting that the VS can generate visual scenes, in addition to processing them as inputs. Furthermore, non-categorical information, about object positions, sizes, etc. is also represented with increasing explicitness in late VS areas V4 and IT(Hong et al., 2016).
This is not necessarily expected in a “pure” object recognition system, as the non-categorical information is not necessary for the categorization task. Thus, these recent findings challenge the long-held object recognition hypothesis of ventral stream and raise the question: What computational objective best explains VS physiology? <span class="citation">(Richards et al. <a href="references.html#ref-Richards:2019bm" role="doc-biblioref">2019</a>)</span></p>
<p>To address that question, we pursued a recently-popularized approach and trained deep neural networks to perform different tasks: we then compared the trained neural networks’ responses to image stimuli to those observed in neurophysiology experiments3-5,8, to see which tasks yielded models that best matched the neural data. We trained our networks to perform one of two visual tasks: a) recognize objects; or b) recognize objects while also retaining enough information about the input image to allow its reconstruction. We studied the evolution of categorical and non-categorical information representations along the visual pathway within these models, and compared that evolution with data from monkey VS. Our main finding is that neural networks optimized for task (b) provide a better match to the representation of non-categorical information in the monkey physiology data than do those optimized for task (a). This suggests that a full understanding of visual ventral stream computations might require considerations other than object recognition.</p>
</div>
<div id="materials-and-methods" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Materials and Methods</h2>
<div id="dataset-and-augmentation" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Dataset and augmentation</h3>
<p>We constructed images of clothing items superimposed at random locations over natural image backgrounds. To achieve this goal, we used all 70,000 images from the Fashion MNIST dataset, a computer vision object recognition dataset comprised of images of clothing articles from 10 different categories. We augmented this dataset by expanding the background of the image two-fold (from 28x28 pixels to 56x56 pixels) and drawing dx and dy linear pixel displacements from a uniform distribution spanning 75% of the image field {-11,11}. Images were then shifted according the randomly drawn dx and dy values. After applying positional shifts, the objects were superimposed over random patches extracted from natural images from the BSDS500 natural image dataset to produce simplified natural scenes which contain categorical (1 of 10 clothing categories) and non-categorical (position shifts) variation. Random 56x56 pixel patches from the BSDS500 dataset were gray scaled before the shifted object images were added to the background patch (Fig 1A). All augmentation was performed on-line during training. That is, every position shift and natural image patch was drawn randomly every training batch instead of pre-computing shifts and backgrounds. This allows every training batch to be composed of unique examples from the dataset and prevents overfitting.</p>

</div>
</div>
</div>
<script type="application/json" class="js-hypothesis-config">

{"showHighlights": "whenSidebarOpen"}

</script>

<script src="https://hypothes.is/embed.js" async></script>
            </section>

          </div>
        </div>
      </div>
<a href="ch-jsr.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-jov.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/elijahc/thesis/edit/master/Rmd//04-chap4.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
