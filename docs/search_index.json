[
["index.html", "Computational Models of Neural Encoding in Vision and Neurostimulation Welcome", " Computational Models of Neural Encoding in Vision and Neurostimulation Elijah Christensen Welcome This is the website for my PhD thesis at University of Colorado, titled “Computational Models of Neural Encoding in Vision and Neurostimulation”. 2020-04-20 "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements Special thanks to Alon Poleg-Polsky for thoughtful discussion and direction and to Gidon Felsen for providing a place to call home. The contribution in Chapter 3 was published in Journal of Sleep Research by Christensen et al. (2019). EC, JZ, JAT and AA were responsible for conception and design of the study. AA acquired the data. EC and JZ conceived the model and developed the code for its application. EC, JZ and JAT analyzed and interpreted the data. EC and JAT drafted the manuscript. EC, AA, JZ and JAT critically revised the manuscript. The contribution in Chapter 5 was published in Journal of Vision by Kindel, Christensen, and Zylberberg (2019). WK and JZ conceived the original project. EC and WK analyzed and interpreted the data. WK and EC drafted the manuscript. EC, WK, and JZ critically revised the manuscript. I would also like to thank my support by the Department of Defense (DoD) through the National Defense Science &amp; Engineering Graduate Fellowship (NDSEG) Program as well as by the Canada First Research Excellence Fund (CFREF) and York University. "],
["list-of-abbreviations.html", "List of Abbreviations", " List of Abbreviations ANN Artificial Neural Network (D)CNN (Deep) Convolutional Neural Network DBS Deep Brain Stimulation LGN Lateral Geniculate Nucleus of the Thalamus ML Machine Learning V1 Primary Visual Cortex V2 Visual Cortex Area V2 "],
["ch-intro.html", "CH 1 Introduction 1.1 Deep Brain Stimulation 1.2 Neural Interfaces 1.3 Summary", " CH 1 Introduction Neurological and psychiatric disease represent a significant societal burden in both advanced and developing countries (Collins et al. 2011) and there is a significant need for more effective treatments. Recent advances in brain stimulation and recording technology have enabled development of long-desired treatment options for many of these diseases in the form of implantable devices that directly stimulate populations of neurons. Deep brain stimulation (DBS) is one such implantable device wherein patients receive electrical pulses via electrodes that have been implanted in the brain to mitigate their disease symptoms. DBS has become an established therapy for movement disorders (Parkinson’s Disease (PD) and essential tremor) (Perlmutter and Mink 2006) as well as epilepsy and psychiatric diseases (Holtzheimer and Mayberg 2011). Another group of implantable neurostimulators are neural prosthetics, such as cortical prostheses, which aim to restore sight in patients with congenital or acquired blindness. The body of work presented here makes progress on two unsolved challenges limiting advances in implantable neurostimulators, namely DBS state detection and more accurate cortical encoding of visual stimuli. 1.1 Deep Brain Stimulation DBS uses a surgically implanted stimulator to apply electrical pulses directly to the brain to mitigate symptoms of neurologic and psychiatric diseases. Historically, drugs have been the primary method of treating these diseases, but DBS has emerged as a promising alternative for patients who do not respond to pharmacotherapy. Parkinson’s disease (PD) was among the first FDA approved uses of DBS for mitigating the disease’s motor symptoms. When employed for treating PD, current best practice for DBS therapy uses constant stimulation even though its therapeutic benefits to motor symptoms are only needed when the patient is awake and trying to move (Fig 1.1). Current implanted stimulators are used this way because they have no way to detect when stimulation is not needed, such as when the patient is asleep or when lower levels of stimulation are needed to correct resting tremor. This strategy of constant stimulation, or open-loop stimulation, is less power efficient and comes with side effects such as impaired cognition, speech, gait, and balance (Hariz et al. 2008). However, activating DBS stimulation only when necessary requires a robust method for discerning whether or not the patient’s brain needs stimulation. For example, a closed-loop DBS system would read out the patient’s brain state and only deliver electrical pulses during periods when the patient is awake. Closed-loop DBS is more power efficient and would have less collateral side effects by only stimulating when necessary. Figure 1.1: Closed-loop DBS system would read out the patient’s brain state to modulate stimulation intensity accordingly based on if the patient is awake, stationary, or moving 1.2 Neural Interfaces Cortical prosthetics (Fig 2) are a form of neural interface used to restore sight in blind patients (Lorach et al. 2013). These implantable neurostimulators bypass lost or damaged neurons by stimulating the damaged neurons targets the same way the original neurons otherwise would. Cortical prostheses must reproduce the neural activity patterns that would typically be relayed naturally by neurons of the thalamic lateral geniculate nucleus (LGN) and retina when directly stimulating visual cortex. Neural encoding, our understanding of how neurons reformat and represent visual stimuli, is key to this goal of properly restoring sight.Unfortunately, current models of neural encoding still struggle to accurately predict neuron responses to natural images stimuli. 1.2.1 Bottom-up neural encoding models The ultimate test of our knowledge of neural encoding is to predict neural responses to stimuli. For instance, we know the visual cortex receives complex spatiotemporal patterns of light relayed by the retina and reformats these patterns to infer what caused them (i.e. the identity of the object). A neuron’s receptive field (RF) depicts the properties of an image that modulate the neurons activity. RF’s are typically represented in models by linear filters applied at the first stage of processing. The inner product (i.e. dot product) between the filter and corresponding image region predict a given neurons response to that image. The linear RF model was insufficient for predicting several non-linear properties of RGC responses to white noise. Linear-Nonlinear-Poisson (LNP) (Paninski et al., 2004) models are commonly used to predict RGC spike rates in responses to image stimuli. LNP combines a linear spatial filter with a single static non-linearity. The LNP model predicts neural responses well for white noise stimuli but does not generalize well when used to predict responses to natural image stimuli. Generalized Linear Model’s (GLM) (Pillow et al., 2008) improve prediction accuracy by accounting for interactions between RGC’s and substituting the spatial receptive field with a spatiotemporal receptive field. 1.2.2 Top-down neural encoding models The genome likely has insufficient capacity for specifying every neuronal connection (synapse) (Zador, 2019) so what mechanisms ensure that neurons are connected correctly? This has recently been referred to as the “brain wiring problem” (Hassan and Hiesinger, 2015). We’ll be looking specifically at how synaptic wiring is determined in visual cortex. Before the eyes even open, molecular interactions and spontaneous activity of retinal ganglion cells (RGC’s) guide development of the initial “coarse” connectivity between retinal ganglion cells in the eye, neurons of the LGN and primary visual cortex (V1) (Del Rio and Feller, 2006; Katz and Shatz, 1996). After this retinotopic map is established, synaptic connectivity continues refinement but requires environmental stimuli (Pietro Berkes et al., 2011). Identifying this “unifying principle” or computational goal that guides stimuli-dependent refinement of connectivity would help explain the structure of visual representations in V1 and beyond. 1.2.2.1 Sparse coding Shortly after the discovery of simple and complex cells (Hubel and Wiesel, 1959), Horace Barlow proposed efficient coding (Barlow, 1961) as an explanation for the computations performed by neural circuits in sensory cortex. The efficient coding hypothesis posits that the overarching goal of sensory processing is to reduce the high information redundancy in stimuli from the physical environment. This view was strengthened by findings that the Gabor-like receptive fields of simple cells are an optimal basis set for natural scenes when optimizing for 1) representation sparsity and 2) image reconstruction (D. Field, 1987; Olshausen and D. J. Field, 1996). Due to the highly metabolic nature of neurons, sparse coding was proposed as an alternative goal which has the added benefit of being metabolically and information efficient (Levy and Baxter, 1996). Sparse coding models were particularly influential after successfully predicting aspects of neural computations in retina (Atick and Redlich, 1992), thalamus (Dan et al., 1996) and V1 (Olshausen and D. J. Field, 1996). Optimizing for efficient coding would predict information redundancy should decrease as it is processed and relayed by successive visual areas. Information redundancy decreases when the same information can be carried by fewer neurons, which occurs as visual information propagates from photoreceptors to RGC and from retinal ganglion cells to cells of area LGN (Figure 1.2). Instead of information redundancy decreasing, as would be predicted by efficient coding, anatomical evidence seems to indicate that information redundancy in primary visual cortex is likely higher than it is prior areas of visual processing (Felleman and Van Essen 1991, @Barlow:2001ub). Furthermore, despite some modest successes at explaining the complex response properties of V2 (the next visual area after V1) (Lee, Ekanadham, and Ng 2008, @Olshausen:2001we) subsequent findings (Berkes, White, and Fiser 2009, @Willmore:2011ks) have shown that subsequent visual areas beyond V2 cannot be explained by the efficient coding hypothesis. Efficient coding alone as an objective is not sufficient for explaining response properties of neurons in higher level visual areas like V4 and inferior temporal cortex (IT). Figure 1.2: Channel capacity, the number of neurons carrying visual information, significantly varies along the ventral visual stream 1.2.2.2 Goal-directed convolutional neural networks Barlow, when reflecting later on his original idea (Barlow, 2001) makes a prescient statement, perhaps without knowing it: “We now need to step back and take a more global view of the brain’s task in order to see what lies behind the importance of recognizing redundancy”. Neural networks which optimize behaviorally relevant tasks (Yamins and DiCarlo 2016) have shown state of the art performance at predicting neuronal activity across the ventral visual stream. 1.3 Summary Chapter 2 provides an introduction to artificial neural networks, their similarities and differences to biological neurons, and the machine learning techniques used to train them which serves as a foundation for the technical chapters that follow. In Chapter 3 we demonstrate ANN models as a tool for decoding sleep state in real-time using only the signals available from intracranial DBS electrodes implanted in the basal ganglia of PD patients. Importantly, this model generalizes decoding to patients never seen by the model and may allow new ways to leverage implantable stimulators for therapeutic benefit. Chapter 4 explores how neural networks can be used as a tool for explaining the overarching goals of neural encoding in ventral stream visual processing. This work was motivated by the observation that visual processing areas are reactivated during visualization tasks indicating their dual role in visual processing and regenerating stimuli. Finally, Chapter 5 demonstrates the utility of neural network models as a way to explain response properties of individual neurons. We use a convolutional neural network (CNN) to achieve performance comparable to state of the art at predicting activity of individual neurons evoked by natural image stimuli in macaque V1. Furthermore, we use this model generatively to explain response properties of cells outside of Hubel and Wiesel’s simple- or complex-cell designations. "],
["ch-mlprimer.html", "CH 2 Neuroscience and ML 2.1 Artificial Units 2.2 Layers 2.3 Model Archetypes 2.4 Layer Architectures 2.5 Loss Function 2.6 Learning Rules 2.7 Optimizers 2.8 Summary", " CH 2 Neuroscience and ML Despite the importance of computers for conducting machine learning and computational neuroscience research both fields had origins long before contemporary transistor computers. In 1943, inspired by the “all-or-none” nature of neural activity, Warren McCulloch (neuroscientist) and Walter Pitts (logician) formalized a simple mathematical definition of a neuron (McCulloch and Pitts 1943). McCulloch and Pitts neurons became the fundamental unit of artificial neural networks (ANN). These artificial neurons, often referred to as (artificial) units, reproduce several key properties of real neurons (Figure 2.1 ). Biological neurons receive input from many other neurons via connections (e.g. synapses) to its dendrites. These synaptic inputs are summated at the soma where the net dendritic input increases or decreases the neurons membrane potential (Fig 2.1). If the net dendritic input shifts the membrane potential beyond a certain threshold (e.g. the threshold potential) the neuron will fire action potentials. Figure 2.1: Synaptically connected neurons in the brain are the substrate of neural computation in brains 2.1 Artificial Units Artificial units (Fig 2.2B) are the basic building block of artificial neural networks. Each artificial unit receives input represented as a sequence of inputs \\(x_i\\) and each input has a corresponding synaptic weight \\(w_i\\). The membrane potential \\(z\\) of the artificial unit is represented by adding net dendritic input to a scalar bias term \\(b\\) which represents intrinsic excitability. Finally, the threshold non-linear response of biological neurons is captured by passing the units membrane potential \\(z\\) through an activation function \\(g\\) which gives the unit output firing rate, or “activation”, \\(a\\). \\[ \\begin{equation} a = g(z) = g(\\sum_{i}{w_i x_i}+b) \\tag{2.1} \\end{equation} \\] 2.2 Layers Just as the brain is comprised of more than one neuron, most models make use of many artificial units. Similar to the laminar organization of the neocortex, artificial neural networks (ANN) group individual units together in layers (Fig 2.2C). The artificial units within a layer collectively operate on a shared input and the layers output consists the collective activations of its constituent artificial units. ANN layers in a model between the inputs (x) and final outputs (y) are often referred to as “hidden” layers. The layers of an ANN are often considered analogous to a population of neurons in regions of the brain which perform similar functions. For instance, primary visual cortex (V1) contain a population of neurons which receive visual inputs from the retina (relayed by LGN). As a population of neurons, V1 processes this visual input and this processed visual information is then relayed to area V2 for subsequent processing. Figure 2.2: Real neurons and artificial units 2.3 Model Archetypes Deep artificial neural network models typically have multiples of these layers stacked one after the other, such that the outputs of one layer become the inputs for the subsequent layer. Deep ANN models are often constructed for a specific purpose, or to perform a specific task. Models are often categorized based on purported task and the structure of the inputs it uses to accomplish this task. For instance, many computer vision researchers train models which, given an image, categorize the object in the image. 2.3.1 Classifiers Classifiers are a class of models that attempt to predict the best category that describes the input from a discrete number of categories. For example, a classic machine learning exercise has been to train a model to predict the category of an object depicted in an image. MNIST, Fashion-MNIST (Xiao, Rasul, and Vollgraf 2017), CIFAR10/100 (Krizhevsky and Hinton 2009) and ImageNet are examples of large labeled image datasets that have been historically popular for evaluating a model’s classification performance. Classifiers are not specific image tasks and can be used on any discrete labeling task. For instance, in Chapter 3 we trained an ANN classifier to predict behavioral sleep state in human PD patients based on features from local field potential spectral decompositions. 2.3.2 Regressors Regressors use their inputs and attempt to predict a continuous value purportedly derived from the input. Recently, neural network models have been used as functional models of the visual system. These models use images to predict neuronal firing rates observed in animals after viewing the same image and has been used to successfully for predicting stimulus evoked activity in retina (McIntosh et al. 2016) and Inferior Temporal cortex (IT) (Yamins et al. 2014). We successfully utilized a convolutional neural network regressor model to predict firing responses for populations of neurons in macaque primary visual cortex (V1) which is the subject of Chapter 5. 2.3.3 Autoencoders Autoencoders are a special class of models which attempt to predict their inputs. This is a trivial task if each the intermediate hidden layers have similar dimensionality as the input and output; the model can simply learn to copy the input into the output. Instead, these models are more often configured to have far few dimensions in their hidden layers. In this configuration the only way to successfully perform the task is to exploit information redundancy in the input to compress the input while retaining as much information as possible. 2.4 Layer Architectures Training an ANN model using machine learning typically requires three components. These components are 1) the model’s layer architecture, 2) objective or loss function, and 3) the models learning rules. The layer architecture of a model explicitly specifies how the artificial units, organized in layers, are connected from input to output. There are a wide variety of layers to choose from when constructing a deep ANN but for the sake of brevity only descriptions of layer architectures used in this work will be provided. 2.4.1 All-to-all All-to-all layers are the simplest and oldest of layer architectures. In all-to-all layers, every input is connected to every unit in the layer. We can modify the previous equation to describe the entire layer instead of a single unit. Mathematically, we represent the layers input and ouput firing rates as a vector of the individal unit activations \\((a_j)\\) where each element \\(j\\) of the vector is a single units activation. By extension, we change the inputs from a scalar to the vector \\((x_i)\\) where each element \\(i\\) is an input. Multiplying the input \\(x_i\\) by the weight matrix \\(w_{i,j}\\) gives an activation of output of length \\(j\\): \\[ a_j = g(z_j) = g(x_i \\cdot w_{i,j} + b_j) \\] 2.5 Loss Function Loss or cost functions ( \\(J\\) ) are mathematical definitions of the goal of the learning system. The loss function is used to calculate a scalar metric quantifying the models’ task performance as a function of its output. Loss functions can take any form mathematically as long as they are 1) differentiable and 2) convex. Reconstruction error (sum of squared pixel errors) were traditionally used for training models which attempt to generate a particular image. We can express the sum squared pixel loss between an image \\(\\hat{y}\\) and the target reference \\(y\\) as: \\[ J(y,\\hat{y}) = \\sum (y-\\hat{y})^2 \\] Objective functions don’t have to depend on a particular dataset or task. For instance, sparse coding models use activation sparseness and reconstruction error as their objective function to learn sparse representations. When minimized over images of natural scenes they learn a set of basis functions that resemble localized receptive fields of simple cells in primary visual cortex (Olshausen and Field 1996) 2.6 Learning Rules Once a model’s architecture and objective function are specified “training” it is simply optimizing the parameters of each layer to improve its loss. The algorithm for how to iteratively update the ANN model parameters to minimize loss was first demonstrated by Rumelhart (Rumelhart, Hinton, and Williams 1986) and it is a simple 2 step process: Forward pass: Use a batch of \\(x\\) input values to calculate the predicted outputs \\(\\hat{y}\\) Backpropogation: Use prediction error to update weights and biases To illustrate this process, we will derive it for a simple 2-layer ANN. For simplicity, we change notation when describing deep ANN with multiple layers such that variable and function subscripts denote the variable or function’s corresponding layer NOT matrix or vector dimensions. For instance, we define the output activations of the lth layer in a model comprised of sequentially stacked all-to-all layers as: \\[ a_l = g(a_{l-1} \\cdot W_l + b_l) \\] 2.6.1 Forward Pass First, we pass a batch of training example inputs (\\(x\\)) through the model to get a batch of output classifications (\\(\\hat{y}\\)). Given our simple feedforward layer defined above, the full equation for the models output is given by: \\[ \\hat{y} = g(W_2 \\cdot g(W_1 \\cdot x +b_1) + b_2) \\] Our loss function \\(J\\) defines how to evaluate the model’s performance as a function of the model’s predicted and target values. The target value is also sometimes referred to as the teaching signal, as it’s used to teach the model the correct output for a given input. For this example, we’ll use sum-squared-error: \\[ J(y,\\hat{y}) = \\sum (y-\\hat{y})^2 \\] 2.6.2 Backpropagation To derive the gradient of the loss function with respect to the model parameters (\\(\\nabla_{\\theta} J\\)) we take a partial derivative of the objective function with respect to the models parameters: \\[ \\nabla_{\\theta} J = \\frac{\\partial J(\\theta ; y, \\hat{y})}{\\partial \\theta} \\] 2.7 Optimizers Once we know the gradient of each weight with respect to the loss, we simply need to adjust the weights of the model in the direction specified by the weight gradient. Continually descending the gradient of the loss function should theoretically result in the optimal weights for performing the model’s task. 2.8 Summary The purpose of this chapter is not to exhaustively cover the field of machine learning but instead to serve as a brief primer of concepts and terms you will encounter in subsequent chapters. Chapter 2 uses an ANN classifier comprised of fully-connected layers to predict sleep states from LFP spectral decompositions. Chapter 3 utilizes a convolutional autoencoder/classifier hybrid model to test hypotheses about computational objectives employed in primate ventral stream visual representations. Finally, Chapter 4 uses a convolutional neural network (CNN) to directly regress neuronal activity in macaque primary visual cortex. Hopefully, you can appreciate the similarities between artificial neural networks and the biological neural networks that inspired them. If nothing else, remember that using machine learning to train ANN models hinges on three components: Model architecture Loss function Learning rules All three components influence both transient and final model performance. "],
["ch-jsr.html", "CH 3 Predicting sleep state in human PD patients1 Abstract 3.1 Introduction 3.2 Methods 3.3 Results", " CH 3 Predicting sleep state in human PD patients1 The contents of this chapter are available in Inferring sleep stage from local field potentials recorded in the subthalamic nucleus of Parkinson’s patients.2 [pdf] [Source Code] Abstract Parkinson’s disease (PD) is highly comorbid with sleep dysfunction. In contrast to motor symptoms, few therapeutic interventions exist to address sleep symptoms in PD. Subthalamic nucleus (STN) deep brain stimulation (DBS) treats advanced PD motor symptoms and may improve sleep architecture. As a proof of concept toward demonstrating that STN‐DBS could be used to identify sleep stages commensurate with clinician‐scored polysomnography (PSG), we developed a novel artificial neural network (ANN) that could trigger targeted stimulation in response to inferred sleep state from STN local field potentials (LFPs) recorded from implanted DBS electrodes. STN LFP recordings were collected from nine PD patients via a percutaneous cable attached to the DBS lead, during a full night’s sleep (6–8 hr) with concurrent polysomnography (PSG). We trained a feedforward neural network to prospectively identify sleep stage with PSG‐level accuracy from 30‐s epochs of LFP recordings. Our model’s sleep‐stage predictions match clinician‐identified sleep stage with a mean accuracy of 91% on held‐out epochs. Furthermore, leave‐one‐group‐out analysis also demonstrates 91% mean classification accuracy for novel subjects. These results, which classify sleep stage across a typical heterogenous sample of PD patients, may indicate spectral biomarkers for automatically scoring sleep stage in PD patients with implanted DBS devices. Further development of this model may also focus on adapting stimulation during specific sleep stages to treat targeted sleep deficits. 3.1 Introduction Sleep is crucial to the regulation of physiological and cognitive functions in humans, and when disordered, greatly diminishes quality of life (Giuditta et al., 1995; Pace-Schott and Hobson, 2002) and adversely affects nervous system repair (Brager et al., 2016; Lucke-Wold et al., 2015). Parkinson’s disease (PD) is a neurodegenerative disorder that exhibits a high degree of comorbidity with a wide range of sleep disorders (De Cock et al., 2008; Tekriwal et al., 2017). The diagnosis and treatment of PD primarily focus on the overt motor symptoms (Postuma et al., 2015). However, there is increasing interest in understanding the impact of non-motor symptoms, such as sleep dysfunction, on overall disease burden (Chaudhuri et al., 2006), and in identifying treatments for these symptoms. With the onset of motor fluctuations or break-through tremor despite optimal medical management, subthalamic nucleus (STN) deep brain stimulation (DBS) surgery has become the gold-standard for treating the motor symptoms of advanced PD (Bronstein et al., 2011; Hamani et al., 2004). Interestingly, several studies have found that STN-DBS can improve sleep in PD (Arnulf et al., 2000; De Cock et al., 2011; Iranzo et al., 2002). In our previous work, using local field potentials (LFP) recorded from DBS electrodes implanted in STN for the treatment of PD, we identified unique spectral patterns within STN oscillatory activity that correlated with distinct sleep cycles—a finding that might offer insight into sleep dysregulation (Thompson et al., 2018). One extension of this work was to determine whether LFP information recorded from STN could be used in real-time to objectively identify sleep cycles for targeted therapy using DBS. In other words, the sleep benefit derived from STN stimulation could potentially be optimized using an adaptive stimulation algorithm that is aimed at specific sleep stages. In this study, we demonstrate the use of a feedforward artificial neural network that predicts sleep stage from LFP recordings, within STN, with high precision. 3.2 Methods 3.2.1 Patient Demographics This study was approved by the Institutional Review Board of the University of Minnesota, where the surgical and recording procedures were performed. All consented study subjects (n = 9) carried a diagnosis of idiopathic PD (Figure 3.1 A). Subjects were unilaterally implanted in STN with a quadripolar DBS electrode (model #3389: Medtronic Inc., Fridley, MN), per routine surgical protocol (Abosch et al., 2012). Experimental details for the recording setup have been previously published (Thompson et al., 2018). Basic characterization of these data was previously reported in Thompson et al., 2018. Figure 3.1: (A) Demographic data and sleep stage characteristics for Parkinson’s disease subjects participating in this study (n = 9). Percent improvement in PD reflects the change in the Unified Parkinson’s Disease Rating Scale (UPDRS) motor scale before and after DBS surgery.(B) Hypnograms from four representative subjects in this study, indicative of common sleep architecture deficits reported for individuals with PD. (C) Distribution of frequency band power contribution to sleep stage for all subjects. 3.2.2 Signal processing of local field potentials Signal processing of the raw STN LFP signals were previously described in Thompson et al., (2018). Briefly, after preprocessing, the four LFP channels (0, 1, 2 and 3) – one recording from each of the four electrical contacts of the implant - were converted into three bipolar derivations (LFP01, LFP12 and LFP23) by sequentially referencing them. Power spectral density (PSD) was estimated using a Fast Fourier Transform from a 2 s long sliding window (Hamming) with 1 s overlap. The final time-evolving spectra had 15 s time and 0.5 Hz frequency resolution. For each subject, LFP data selected for further analysis were based on the location of the DBS electrode contact within STN and this was verified by the following: (1) intraoperative microelectrode recordings that identified cells with firing characteristics consistent with STN neurons; (2) antiparkinsonian benefit and side effects of macrostimulation; (3) preoperative stereotactic T1- and T2-weighted images merged to a postoperative MRI demonstrating the position of the DBS electrode within the borders of STN; (4) the use of Framelink (Medtronic Corp.) software to analyze DBS position on the postoperative MRI; and (5) evaluation of the efficacy of post-programming stimulation on contralateral motor symptoms for each subject (Ince et al., 2010). Selection of which contact(s) to use for study recordings was based on the STN contact(s) associated with peak beta-spectrum activity as this feature correlates with the optimal programming contact(s) for the treatment of contralateral motor symptoms (Ince et al., 2010). These criteria were used to ensure that the selected contact was most reliably in the same relative anatomical location across patients to permit generalizability of the model. 3.2.3 Video PSG Scoring The polysomnographic electrode montage used was the following: F3–C3, P3–O1, F4–C4 and P4–O2, EOGL–A2, EOGR–A1 and chin EMG (Iber et al., 2007). Sleep stages were determined by analyzing 30 s epochs of the PSG, by a sleep neurologist, with each epoch classified as Awake or as belonging to one of the following sleep stages: rapid eye movement (REM), or the non-REM (NREM) stages of N1, N2 or N3. 3.2.4 Model Description We trained a feedforward artificial neural network (ANN) with a single hidden layer (Fig 2B) to prospectively identify whether a given 30-second epoch of STN-LFP recording took place during one of three possible states: REM, NREM or Awake. Inputs to the model were 8 separate frequency band power bins, averaged over 30 seconds: delta (0-3 Hz), theta (3-7 Hz), alpha (7-13 Hz), low beta (13-20 Hz), high beta (20-30 Hz), and low gamma (30-90 Hz), high gamma (90-200) and high frequency oscillations (200-350). Each frequency range input feature was normalized independently by subtracting the mean and scaling by the variance of feature. The ANN output is a probability that the measured epoch occurs during one of the three possible states. Optimal ANN architecture was chosen based on hyperparameter optimization detailed below. The ANN model utilizes a single hidden layer to encode the normalized spectral power bands within 32 features by calculating weighted sums of the input frequency power and scaling them by a non-linear function. Weighted linear combinations of these 32 features are then used by the network to compute sleep state probabilities with application of a softmax nonlinearity. 3.3 Results 3.3.1 Model Performance and Validation First, we tested the model’s ability to predict sleep stages on novel examples from patients included in the training set. We pooled 80% of each patient’s 30-second STN-LFP recording epochs across all 9 patients to train the model. The remaining 20% of the withheld epochs were used to evaluate the model’s performance on novel examples from familiar patients. The train-test fractions (80:20) were sampled randomly for each patient and performance was averaged in replicates of 5 to prevent sampling bias. The model was able to correctly predict sleep stage from STN-LFP epochs with mean accuracy of 91% (Fig 3.2 A). Figure 3.2: (A) In the “hybrid” strategy a random 80% of each patient’s local field potential (LFP) recordings were pooled to train the model. Model accuracy and Cohen’s κ were evaluated on the withheld 20% from each patient. This analysis was replicated in four other random 80:20 splits to control sampling bias. Cohen’s κ magnitude guidelines derived from (Fleiss and Cohen, 1973). (B) A Leave One Group Out (LOGO) cross validation strategy was used to test generalizability to unseen patients. Each data point represents a model trained with a specific patient excluded from its training data. Model accuracy and Cohen’s κ were evaluated on data from the held-out patient. (C) Confusion matrices of representative models trained using LOGO cross validation strategy. The first two confusion matrices represent individual subjects and the final confusion matrix depicts the fraction of epochs with specific class labels for all subjects. Training a model from scratch for each new patient is often intractable. Therefore, the model’s ability to perform well on never-seen subjects demonstrates its sensitivity to the salient spectral features of sleep across individual variations. To test this level of generalization, the model was trained on all epochs from 8 of the 9 patients. Subsequently, model performance was evaluated on all epochs from the held-out patient. Thus, nine different models were trained, each with a specific patient withheld from its training data. As above, model performance was quantified using accuracy and Cohen’s κ (Fig. 3B). Across all models, mean classification accuracy of 91% was observed. Finally, because the number of epochs of each observed sleep state varies between patients in the dataset, we produced confusion matrices for the test patient of each model and show representative examples from patients with significantly imbalanced sampling as well as summary matrix averaged across all models (Fig 3C). This demonstrates that the model’s error rate varies as a function of sleep stage representation, with less frequent stages showing a higher error rate (see Table 1). Extra footnote↩︎ This chapter was previously published in (Christensen et al. 2019) Journal of Sleep Research and is included with permission from the copyright holder↩︎ "],
["ch-vae.html", "CH 4 Models of the ventral stream that visualize and categorize images Abstract 4.1 Materials and Methods", " CH 4 Models of the ventral stream that visualize and categorize images The work presented in this chapter is not yet published, but a preprint is available (see below) pdf Abstract The ventral stream (VS) of visual cortex begins in primary visual cortex (V1), ends in inferior temporal cortex (IT), and is essential for object recognition. Accordingly, the long-standing belief in the field is that the ventral stream could be understood as mapping visual scenes onto neuronal firing patterns that represent object identity(Felleman and Van Essen, 1991). Supporting that assertion, deep convolutional neural networks (DCNN’s) trained to categorize objects in natural images develop intermediate representations that resemble those in primate VS(Cadieu et al., 2014; Güçlü and van Gerven, 2015; Yamins et al., 2014; Yamins and DiCarlo, 2016). However, several recent findings appear at odds with the object recognition hypothesis. VS and other visual areas are also engaged during visualization of both prior experience and novel scenes (O’Craven and Kanwisher, 2006; Stokes et al., 2009), suggesting that the VS can generate visual scenes, in addition to processing them as inputs. Furthermore, non-categorical information, about object positions, sizes, etc. is also represented with increasing explicitness in late VS areas V4 and IT(Hong et al., 2016). This is not necessarily expected in a “pure” object recognition system, as the non-categorical information is not necessary for the categorization task. Thus, these recent findings challenge the long-held object recognition hypothesis of ventral stream and raise the question: What computational objective best explains VS physiology? (Richards et al. 2019) To address that question, we pursued a recently-popularized approach and trained deep neural networks to perform different tasks: we then compared the trained neural networks’ responses to image stimuli to those observed in neurophysiology experiments3-5,8, to see which tasks yielded models that best matched the neural data. We trained our networks to perform one of two visual tasks: a) recognize objects; or b) recognize objects while also retaining enough information about the input image to allow its reconstruction. We studied the evolution of categorical and non-categorical information representations along the visual pathway within these models, and compared that evolution with data from monkey VS. Our main finding is that neural networks optimized for task (b) provide a better match to the representation of non-categorical information in the monkey physiology data than do those optimized for task (a). This suggests that a full understanding of visual ventral stream computations might require considerations other than object recognition. 4.1 Materials and Methods 4.1.1 Dataset and augmentation We constructed images of clothing items superimposed at random locations over natural image backgrounds. To achieve this goal, we used all 70,000 images from the Fashion MNIST dataset, a computer vision object recognition dataset comprised of images of clothing articles from 10 different categories. We augmented this dataset by expanding the background of the image two-fold (from 28x28 pixels to 56x56 pixels) and drawing dx and dy linear pixel displacements from a uniform distribution spanning 75% of the image field {-11,11}. Images were then shifted according the randomly drawn dx and dy values. After applying positional shifts, the objects were superimposed over random patches extracted from natural images from the BSDS500 natural image dataset to produce simplified natural scenes which contain categorical (1 of 10 clothing categories) and non-categorical (position shifts) variation. Random 56x56 pixel patches from the BSDS500 dataset were gray scaled before the shifted object images were added to the background patch (Fig 1A). All augmentation was performed on-line during training. That is, every position shift and natural image patch was drawn randomly every training batch instead of pre-computing shifts and backgrounds. This allows every training batch to be composed of unique examples from the dataset and prevents overfitting. "],
["ch-jov.html", "CH 5 Predicting single unit responses in macaque V1 Abstract 5.1 Introduction 5.2 Methods", " CH 5 Predicting single unit responses in macaque V1 The contents of this chapter are available in Using deep learning to probe the neural code for images in primary visual cortex3 [Source Code] Abstract Primary visual cortex (V1) is the first stage of cortical image processing, and major effort in systems neuroscience is devoted to understanding how it encodes information about visual stimuli. Within V1, many neurons respond selectively to edges of a given preferred orientation: These are known as either simple or complex cells. Other neurons respond to localized center–surround image features. Still others respond selectively to certain image stimuli, but the specific features that excite them are unknown. Moreover, even for the simple and complex cells—the best-understood V1 neurons—it is challenging to predict how they will respond to natural image stimuli. Thus, there are important gaps in our understanding of how V1 encodes images. To fill this gap, we trained deep convolutional neural networks to predict the firing rates of V1 neurons in response to natural image stimuli, and we find that the predicted firing rates are highly correlated (\\(CC_{norm}\\) = 0.556 ± 0.01) with the neurons’ actual firing rates over a population of 355 neurons. This performance value is quoted for all neurons, with no selection filter. Performance is better for more active neurons: When evaluated only on neurons with mean firing rates above 5 Hz, our predictors achieve correlations of \\(CC_{norm}\\) = 0.69 ± 0.01 with the neurons’ true firing rates. We find that the firing rates of both orientation-selective and non-orientation-selective neurons can be predicted with high accuracy. Additionally, we use a variety of models to benchmark performance and find that our convolutional neural-network model makes more accurate predictions. 5.1 Introduction Our ability to see arises because of the activity evoked in our brains as we view the world around us. Ever since Hubel and Wiesel (1959) mapped the flow of visual information from the retina to thalamus and then cortex, understanding how these different regions encode and process visual information has been a major focus of visual systems neuroscience. In the first cortical layer of visual processing—primary visual cortex (V1)—Hubel and Wiesel identified neurons that respond to oriented edges within image stimuli. These are called simple or complex cells, depending on how sensitive their responses are to shifts in the position of the edge. The simple and complex cells are well studied (Lehky, Sejnowski, &amp; Desimone, 1992; David, Vinje, &amp; Gallant, 2004; Montijn, Meijer, Lansink, &amp; Pennartz, 2016). However, many V1 neurons are neither simple nor complex cells, and the classical models of simple and complex cells often fail to predict how those neurons will respond to naturalistic stimuli (Olshausen &amp; Field, 2005). Thus, much of how V1 encodes visual information remains unknown. We use deep learning to address this longstanding problem. 5.2 Methods 5.2.1 Experimental Data Neural activity was recorded in monkeys’ V1 as they were shown a series of images (Fig 5.1A). The image set contains 270 circularly cropped natural images (Fig 5.1B). (C) The response of a single neuron over repeated presentations of an image. Ticks indicate the neuron’s spiking; each row corresponds to a different image-presentation trial. During the response window, the firing rate is computed and then averaged over trials to yield the average response \\(A_{n,i}\\) used in our analysis. (D) The neuron responds to image stimuli with a latency of ∼50 ms from the image onset at t = 0, as seen in the peristimulus time histogram (firing rate plotted against time, averaged over all 270 images). Figure 5.1: Experimental data collection and processing. This chapter was previously published in (Kindel, Christensen, and Zylberberg 2019) Journal of Vision and is included with permission from the copyright holder↩︎ "],
["ch5-discussion.html", "CH 6 Single Units to Brain-wide States Abstract 6.1 Modeling Neural Encoding with ANN’s 6.2 Deep Brain Stimulation 6.3 ANN models are useful across a wide range of physiologic scales", " CH 6 Single Units to Brain-wide States Abstract A persisting challenge in neuroscience is to bridge the gap between the complex tasks we know brains can perform and the physical components (neurons) that enable them. In vision, this divide is particularly wide, and much effort has been devoted to understanding how our brain processes visual information. For instance, we know the visual cortex receives complex spatiotemporal patterns of light relayed by the retina and reformats these patterns to infer what caused them (i.e. the identity of the object present) (DiCarlo, Zoccolan, and Rust 2012). Answering this question requires first understanding how visual information is encoded at each sequential stage of processing along brain areas in visual cortex. Computational modeling has much to offer neuroscience in addressing this knowledge gap. One approach is to build computational models to replicate the neural encoding which takes place when the brain receives sensory stimuli. 6.1 Modeling Neural Encoding with ANN’s 6.1.1 Predicting Single Units At the most granular level, information is encoded in the spiking activities of individual neurons. Traditional systems neuroscience approaches have advanced our understanding neural encoding by providing explanations for what individual neurons compute. In visual systems neuroscience, Hubel and Wiesel performed the seminal work in the field showing that individual simple and complex cells in primary visual cortex (V1) “tuned” to respond to oriented edges. This approach has worked well for cells that respond well to simple stimuli, but the encoding properties of many other cells in V1 are still unexplained (Olshausen and Field 2006). We demonstrate ANN’s trained with machine learning techniques are a robust model of neural encoding in V1 capable of accurately predicting single neuron responses to natural stimuli. Importantly, this model achieves equally good predictability for both orientation selective and non-orientation selective cells for natural image stimuli. We show this approach is a useful tool for studying responses properties of previously difficult to study cells in V1. 6.1.2 Objectively Useful David Marr proposed the idea that understanding the computational goals of visual processing is equally important and complementary to an understanding of the parts (e.g. individual neurons) that physically implement it. For early visual areas, efficient coding as a computational goal has been successful at explaining response properties of cells in primary visual cortex (V1) but has not worked as well at explaining responses in higher visual areas. The prevailing view for higher visual areas like inferior temporal cortex (IT) is that object recognition best describes its objective but directly testing this hypothesis is difficult. Questions of this nature are fundamentally challenging to test, especially when limited to only analyzing responses for a handful of neurons. Recent results have demonstrated ANN’s may be better suited for evaluating higher visual area objectives. Deep convolutional neural networks (DCNN’s) trained to categorize objects in images also develop internal representations which also match IT responses to natural images. It has been posited that matching representations could only arise if both the model and ventral stream are optimized for the same objective. We tested this explanation by optimizing models for both image categorization and a composite categorize and reconstruct objective. We find models which optimize the composite objective have representations which IT better than categorize alone. This is surprising, if strictly object recognition best describes the objective of visual processing in the ventral stream, optimizing an alternate objective should develop more poorly matching representations. However, this finding may help reconcile two observations at odds with the strictly object recognition hypothesis. First, it’s been shown that visual processing areas show matching activation patterns in response to both viewing a scene and mentally visualizing the same scene (Stokes et al. 2009, @OCraven:2000th, @Freud:2016dj, @Sereno:2011hq). Second, ventral stream areas explicitly retain information not useful for object recognition(Hong et al. 2016). Half of nonhuman primate neocortex is devoted to visual processing (Felleman and Van Essen 1991), underscoring both its complexity and evolutionary importance. Furthermore, the ‘No free lunch theorems’ demonstrate objective function choice is not arbitrary; no learning algorithm performs well on all tasks. These pressures dictate an alternate objective choice, should have a compelling advantage. Our work suggests that an advantage such as noise robustness might explain why the alternate categorize and reconstruct objective provides a better match to IT representations. 6.2 Deep Brain Stimulation Current neurostimulators are non-adaptive, limited by lack of robust methods for reading out a patient’s brain state necessary for adaptive neurostimulation. This work has also shown the promise of ANN’s as useful tool decoding information from neural activity. In chapter 3 we described our efforts to build such a model capable of detecting sleep stage from local field potential (LFP) recordings taken from DBS electrodes implanted in the subthalamic nucleus (STN). In this work we trained an ANN classifier model to predict the sleep state of PD patients from spectral decomposition features in their local field potentials. 6.3 ANN models are useful across a wide range of physiologic scales In this work we leverage ANN models as a powerful tool to improve our understanding of neural encoding, predicting brain-wide states, replicating population response properties in IT, and predicting individual neuron responses to stimuli. We demonstrate the objective best describing higher visual areas may be more complex than solely object recognition; and show why this may be important for practical reasons. While these findings have extended our understanding of visual encoding, there is still important work to be done to make more accurate models of cortical visual processing. For instance, the brain’s visual processing system is optimized to operate on sequences of images (e.g. video) and not just a single snapshot. The model’s described in this work do not take advantage of extra information present in the time domain of video and future models will likely take advantage of this kind of information. "],
["references.html", "REFERENCES", " REFERENCES Barlow, H. 2001. “Redundancy reduction revisited.” Network (Bristol, England) 12 (3): 241–53. Berkes, Pietro, Ben White, and József Fiser. 2009. “No evidence for active sparsification in the visual cortex,” 108–16. Christensen, Elijah, Aviva Abosch, John A Thompson, and Joel Zylberberg. 2019. “Inferring sleep stage from local field potentials recorded in the subthalamic nucleus of Parkinson’s patients.” Journal of Sleep Research 28 (4): e12806. Collins, Pamela Y, Vikram Patel, Sarah S Joestl, Dana March, Thomas R Insel, Abdallah S Daar, Scientific Advisory Board and the Executive Committee of the Grand Challenges on Global Mental Health, et al. 2011. “Grand challenges in global mental health.” Nature 475 (7354): 27–30. DiCarlo, James J, Davide Zoccolan, and Nicole C Rust. 2012. “How does the brain solve visual object recognition?” Neuron 73 (3): 415–34. Felleman, D J, and D C Van Essen. 1991. “Distributed hierarchical processing in the primate cerebral cortex.” Cerebral Cortex (New York, N.Y. : 1991) 1 (1): 1–47. Freud, Erez, David C Plaut, and Marlene Behrmann. 2016. “’What’ Is Happening in the Dorsal Visual Pathway.” Trends in Cognitive Sciences 20 (10): 773–84. Hariz, Marwan I, Stig Rehncrona, Niall P Quinn, Johannes D Speelman, Carin Wensing, and Multicentre Advanced Parkinson’s Disease Deep Brain Stimulation Group. 2008. “Multicenter study on deep brain stimulation in Parkinson’s disease: an independent assessment of reported adverse events at 4 years.” Movement Disorders : Official Journal of the Movement Disorder Society 23 (3): 416–21. Holtzheimer, Paul E, and Helen S Mayberg. 2011. “Deep brain stimulation for psychiatric disorders.” Annual Review of Neuroscience 34 (1): 289–307. Hong, Ha, Daniel L K Yamins, Najib J Majaj, and James J DiCarlo. 2016. “Explicit information for category-orthogonal object properties increases along the ventral stream.” Nature Neuroscience 19 (4): 613–22. Kindel, William F, Elijah D Christensen, and Joel Zylberberg. 2019. “Using deep learning to probe the neural code for images in primary visual cortex.” Journal of Vision 19 (4): 29. Krizhevsky, A, and G Hinton. 2009. “Learning multiple layers of features from tiny images.” Lee, Honglak, Chaitanya Ekanadham, and Andrew Y Ng. 2008. “Sparse deep belief net model for visual area V2,” 873–80. Lorach, Henri, Olivier Marre, José-Alain Sahel, Ryad Benosman, and Serge Picaud. 2013. “Neural stimulation for visual rehabilitation: advances and challenges.” Journal of Physiology, Paris 107 (5): 421–31. McCulloch, W S, and W Pitts. 1943. A logical calculus of the ideas immanent in nervous activity. Vol. 52. McIntosh, Lane T, Niru Maheswaranathan, Aran Nayebi, Surya Ganguli, and Stephen A Baccus. 2016. “Deep Learning Models of the Retinal Response to Natural Scenes.” Advances in Neural Information Processing Systems 29: 1369–77. O’Craven, K M, and N Kanwisher. 2000. “Mental imagery of faces and places activates corresponding stiimulus-specific brain regions.” Journal of Cognitive Neuroscience 12 (6): 1013–23. Olshausen, B A, and D J Field. 1996. “Emergence of simple-cell receptive field properties by learning a sparse code for natural images.” Nature 381 (6583): 607–9. Olshausen, B A, P Sallee, and MS Lewicki. 2001. “Learning sparse image codes using a wavelet pyramid architecture.” Papers.nips.cc Papers.nips.cc. Olshausen, Bruno A, and David J Field. 2006. “How Close Are We to Understanding V1?” Dx.doi.org.proxy.hsl.ucdenver.edu 17 (8): 1665–99. Perlmutter, Joel S, and Jonathan W Mink. 2006. “Deep brain stimulation.” Annual Review of Neuroscience 29: 229–57. Richards, Blake A, Timothy P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz, Amelia Christensen, Claudia Clopath, et al. 2019. “A deep learning framework for neuroscience.” Nature Neuroscience 22 (11): 1761–70. Rumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning representations by back-propagating errors.” Nature 323 (6088): 533–36. Sereno, Anne B, and Sidney R Lehky. 2011. “Population coding of visual space: comparison of spatial representations in dorsal and ventral pathways.” Frontiers in Computational Neuroscience 4: 159. Stokes, Mark, Russell Thompson, Rhodri Cusack, and John Duncan. 2009. “Top-down activation of shape-specific population codes in visual cortex during mental imagery.” The Journal of Neuroscience : The Official Journal of the Society for Neuroscience 29 (5): 1565–72. Willmore, Ben D B, James A Mazer, and Jack L Gallant. 2011. “Sparse coding in striate and extrastriate visual cortex.” Journal of Neurophysiology 105 (6): 2907–19. Xiao, Han, Kashif Rasul, and Roland Vollgraf. 2017. “Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms,” August. http://arxiv.org/abs/1708.07747. Yamins, Daniel L K, and James J DiCarlo. 2016. “Using goal-driven deep learning models to understand sensory cortex.” Nature Neuroscience 19 (3): 356–65. Yamins, Daniel L K, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J DiCarlo. 2014. “Performance-optimized hierarchical models predict neural responses in higher visual cortex.” Proceedings of the National Academy of Sciences of the United States of America 111 (23): 8619–24. "]
]
